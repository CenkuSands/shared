input {
    file {
        path => "/path/to/your/csv/files/*.csv"  # The path to your CSV file(s)
        start_position => "beginning"
        sincedb_path => "/dev/null"  # Ensures Logstash reads the file from the beginning each time
        codec => plain {
            charset => "UTF-8"
        }
    }
}

filter {
    csv {
        separator => ","  # Your CSV uses a comma to separate fields
        columns => ["timestamp", "topic", "size"]  # Define the columns for your CSV data
    }
    
    # Convert the 'size' field to an integer (as it represents a size)
    mutate {
        convert => { "size" => "integer" }
    }

    # Optionally, if you want to convert timestamp to an Elasticsearch-friendly format
    date {
        match => ["timestamp", "yyyy-MM-dd HH:mm:ss"]
        target => "@timestamp"
        remove_field => ["timestamp"]  # Optional: Remove the original timestamp field if you want
    }
}

output {
    elasticsearch {
        hosts => ["http://localhost:9200"]  # Elasticsearch server (adjust IP if needed)
        index => "kafka-topics-%{+YYYY.MM.dd}"  # Customize your index name
    }
    stdout { codec => rubydebug }  # Debug output to console
}
